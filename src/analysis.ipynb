{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/anlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "import pandas as pd\n",
    "from langchain_core.documents import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain import PromptTemplate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\", model_kwargs={\"device\": \"cuda\"})\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", model_kwargs={\"device\": \"cuda\"})\n",
    "vectorstore = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_inference(qa_chain, questions, batch_size, ground_truths=None):\n",
    "    \"\"\"\n",
    "    Processes the list of questions in batches and returns accumulated predictions.\n",
    "    If ground_truths are provided, computes and prints metrics for each batch.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    num_batches = (len(questions) + batch_size - 1) // batch_size\n",
    "\n",
    "    for i in range(0, len(questions), batch_size):\n",
    "        batch = questions[i:i+batch_size]\n",
    "        batch_inputs = [{\"query\": q} for q in batch]\n",
    "        # Use invoke to process the batch\n",
    "        with torch.no_grad():\n",
    "            if batch_size > 1:\n",
    "                batch_outputs = qa_chain.batch(batch_inputs)\n",
    "            else:\n",
    "                batch_outputs = [qa_chain.invoke(batch_inputs[0])]\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        batch_predictions = []\n",
    "        for output in batch_outputs:\n",
    "            # Handle different output formats (dict or string)\n",
    "            if isinstance(output, dict):\n",
    "                answer = output.get(\"result\", output)\n",
    "            else:\n",
    "                answer = output\n",
    "            batch_predictions.append(answer)\n",
    "        \n",
    "        predictions.extend(batch_predictions)\n",
    "        \n",
    "        # If ground truths are provided, compute and print batch metrics\n",
    "        if ground_truths:\n",
    "            batch_ground_truths = ground_truths[i:i+batch_size]\n",
    "            batch_metrics = calculate_metrics(batch_predictions, batch_ground_truths)\n",
    "            print(f\"Batch {i//batch_size + 1}/{num_batches} Metrics:\")\n",
    "            for key, value in batch_metrics.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "    return predictions\n",
    "def remove_punctuation(sentence):\n",
    "    if sentence[-1] == \".\":\n",
    "        return sentence[:-1]\n",
    "    else:\n",
    "        return sentence\n",
    "\n",
    "def calculate_metrics(predictions, ground_truths):\n",
    "    \"\"\"\n",
    "    Calculates evaluation metrics (recall, F1, exact match) for a list of predictions\n",
    "    compared to ground truths.\n",
    "    \"\"\"\n",
    "    # Ensure the lengths match\n",
    "    assert len(predictions) == len(ground_truths), \"Lengths of predictions and ground truths must match.\"\n",
    "    total_examples = len(predictions)\n",
    "    \n",
    "    exact_match_count = 0\n",
    "    total_recall = 0.0\n",
    "    total_precision = 0.0\n",
    "\n",
    "    for pred, gt in zip(predictions, ground_truths):\n",
    "        pred = pred.lower()\n",
    "        gt = gt.lower()\n",
    "        # Count exact matches\n",
    "        if pred == gt:\n",
    "            exact_match_count += 1\n",
    "        \n",
    "        # Tokenize the prediction and ground truth\n",
    "        pred_tokens = set(pred.split())\n",
    "        gt_tokens = set(gt.split())\n",
    "        \n",
    "        # Compute intersection of tokens\n",
    "        common_tokens = pred_tokens & gt_tokens\n",
    "        \n",
    "        # Compute recall and precision for the current pair\n",
    "        recall = len(common_tokens) / len(gt_tokens) if gt_tokens else 0\n",
    "        precision = len(common_tokens) / len(pred_tokens) if pred_tokens else 0\n",
    "        \n",
    "        total_recall += recall\n",
    "        total_precision += precision\n",
    "\n",
    "    # Compute average recall and precision\n",
    "    avg_recall = total_recall / total_examples\n",
    "    avg_precision = total_precision / total_examples\n",
    "    \n",
    "    # Calculate F1 score (harmonic mean)\n",
    "    f1_score = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) if (avg_precision + avg_recall) > 0 else 0\n",
    "    exact_match = exact_match_count / total_examples\n",
    "\n",
    "    return {\n",
    "        \"recall\": avg_recall,\n",
    "        \"f1\": f1_score,\n",
    "        \"exact_match\": exact_match,\n",
    "        \"exact_match_count\": exact_match_count,\n",
    "        \"total_examples\": total_examples,\n",
    "        \"total_recall\": total_recall,\n",
    "        \"total_precision\": total_precision\n",
    "    }\n",
    "def extract_answer(answer):\n",
    "    import re\n",
    "\n",
    "    # Use a regex pattern to capture everything after \"Helpful Answer:\" until the end of the text\n",
    "    match = re.search(r\"(?i)Helpful Answer:\\s*(.*)\", answer, re.DOTALL)\n",
    "    if match:\n",
    "        answer_part = match.group(1).strip()\n",
    "        answer_part = answer_part.split(\"\\n\")[0]\n",
    "        return answer_part\n",
    "    else:\n",
    "        return \"Extract failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.80s/it]\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "topk = 3\n",
    "# generation_model_name = \"google/flan-t5-base\"\n",
    "generation_model_name = \"Qwen/Qwen2-7B-Instruct\"\n",
    "ground_truths = None\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": topk})\n",
    "tokenizer = AutoTokenizer.from_pretrained(generation_model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(generation_model_name, device_map=\"auto\")\n",
    "model = AutoModelForCausalLM.from_pretrained(generation_model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "# model.config.use_cache = False\n",
    "model.eval()\n",
    "# pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n",
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer, torch_dtype=torch.float16)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your custom instruction prompt\n",
    "ans_req = \"\"\"Answer the question based only on the provided context in just one sentence. Each answer must be extremely succinct—limited to just several keywords—and should not repeat the question.\"\"\"\n",
    "yes_or_no_req = \"\"\"If a question is a yes or no question, the answer must be exactly 'yes' or 'no' without any additional information.\"\"\"\n",
    "instruction_prompt = \" \".join([ans_req, yes_or_no_req])\n",
    "\n",
    "template = (\n",
    "    f\"{instruction_prompt}\\n\"\n",
    "    \"Context: {context}\\n\"\n",
    "    \"Question: {question}\\n\"\n",
    "    \"Helpful Answer:\"\n",
    ")\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, chain_type_kwargs={\"prompt\": prompt_template})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: {'source': '/home/ubuntu/ThreeRiversRAG/data/retrieve_source/total_web_txt/275.txt'}\n",
      "file: {'source': '/home/ubuntu/ThreeRiversRAG/data/retrieve_source/total_web_txt/275.txt'}\n",
      "file: {'source': '/home/ubuntu/ThreeRiversRAG/data/retrieve_source/crawled_pdf_data/23255_2024_Operating_Budget.txt'}\n",
      "content: faq contact us sustainability sub-menu climate action plan resilient pittsburgh food systems sustainability & resilience library affiliations & memberships environmental planning and review search you are here : home / resident services / community programming / city cuts city cuts we are currently not taking applications from residents but the window will be opening soon! please check back the end of march/beginning of april city cuts is the city of pittsburgh’s lawn cutting program for our\n",
      "content: find the different ways to apply by checking out the right side to \"request city cuts service\". we are currently looking for pittsburgh landscapers and contractors to participate in this program. we are offering competitive rates and will be paying based on sq footage per yard. if you or someone you know is interested, please apply below. apply faqs q - i am a person 62 or older requesting assistance with cutting the grass in my yard, located in the city of pittsburgh. i own my own home. do i\n",
      "content: as they age. in addition, this division oversees the city of pittsburgh’s commitment to nutrition support for city residents through the management of food programs for youth and older adults, the city’s farmers markets, city farms, and the newly created pittsburgh food justice fund. community programs – community programs and events are focused on activating our park and recreation spaces to engage city residents in a whole host of enriching experiences. our office of special events is charged\n"
     ]
    }
   ],
   "source": [
    "question = \"What programs are available through City Cuts community program in Pittsburgh?\"\n",
    "relevant_docs = vectorstore.similarity_search(question, k=3)\n",
    "for doc in relevant_docs:\n",
    "    print(f\"file: {doc.metadata}\")\n",
    "for doc in relevant_docs:\n",
    "    print(f\"content: {doc.page_content}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based only on the provided context in just one sentence. Each answer must be extremely succinct—limited to just several keywords—and should not repeat the question. If a question is a yes or no question, the answer must be exactly 'yes' or 'no' without any additional information.\n",
      "Context: carnegie mellon university\n",
      "\n",
      "scottish terrier [ 10 ] website cmu .edu carnegie mellon university ( cmu ) is a private research university in pittsburgh , pennsylvania, united states. the institution was established in 1900 by andrew carnegie as the carnegie technical schools . in 1912, it became the carnegie institute of technology and began granting four-year degrees. in 1967, it became carnegie mellon university through its merger with the mellon institute of industrial research , founded in 1913 by andrew mellon and\n",
      "\n",
      "and athletics jared l. cohon university center swimming & diving pool tennis courts tepper fitness center varsity weight room visiting team brochure wiegand gym recreation giving shop inside athletics tartan facts who founded carnegie mellon university? carnegie technical schools was founded in 1900 by andrew carnegie. twelve years later it became known as the carnegie institute of technology. in 1967, the school merged with mellon institute and became what is known today as carnegie mellon\n",
      "Question: When was Carnegie Mellon University founded?\n",
      "Helpful Answer: 1900\n",
      "1900\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Combine the instruction prompt with the question.\n",
    "# final_prompt = f\"{instruction_prompt}\\nQuestion: {question}\"\n",
    "\n",
    "predictions = batch_inference(qa_chain, [question], 1, ground_truths if ground_truths else None)\n",
    "print(predictions[0])\n",
    "print(extract_answer(predictions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "yes, there's the climate action plan in place\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'recall': 0.0,\n",
       " 'f1': 0,\n",
       " 'exact_match': 0.0,\n",
       " 'exact_match_count': 0,\n",
       " 'total_examples': 1,\n",
       " 'total_recall': 0.0,\n",
       " 'total_precision': 0.0}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = [\"yes\"]\n",
    "gt = [\"Yes, there's the Climate Action Plan in place.\"]\n",
    "calculate_metrics(ans, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'yeyuan', 'lc_hub_repo': 'rag-prompt-llama', 'lc_hub_commit_hash': '89712f4ba006ef101de75372b01a7bdc9e7184bf681f221070d86243c0a15772'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"[INST]<<SYS>> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use 50 words maximum and keep the answer concise.<</SYS>> \\nQuestion: {question} \\nContext: {context} \\nAnswer: [/INST]\"), additional_kwargs={})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/anlp/lib/python3.11/site-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull(\"yeyuan/rag-prompt-llama\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
